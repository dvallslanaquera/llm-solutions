{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import glob \n",
    "\n",
    "# Function to retrieve text data\n",
    "def load_text_files(directory):\n",
    "    filepaths = glob.glob(os.path.join(directory, '*.txt'))\n",
    "    articles = []\n",
    "    for path in filepaths:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            title = os.path.basename(path).replace('.txt', '')\n",
    "            articles.append({'title': title, 'content': content})\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Example preprocessing: converting to lowercase\n",
    "    data['content'] = data['content'].str.lower()\n",
    "    return data\n",
    "\n",
    "data = load_text_files('data/')\n",
    "data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build a retrieval system by:\n",
    "- Vectorize articles: we will use distilbert, but there are plenty of models such as all-MiniLM-L6-v2, paraphrase-MiniLM-L6-v2, xlm-r-100langs-bert-base-nli-stsb-mean-tokens, etc.\n",
    "- Store vectors using FAISS \n",
    "\n",
    "Approx running time: 7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# use distilbert \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = data['content'].tolist()\n",
    "\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss \n",
    "import numpy as np \n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings))\n",
    "    return index \n",
    "\n",
    "index = build_faiss_index(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement Retrieval-Augmented Generation (RAG)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaissance Periodization\n",
      "Using Performance to Regulate Your Training Volume\n",
      "hey folks dr michael retalia for\n",
      "renaissance periodization using\n",
      "performance to regulate your training\n",
      "volume\n",
      "we already know how to do it from two\n",
      "weeks ago's videos of using the pump\n",
      "we know how to do it from last week's\n",
      "video of using muscle disruption\n",
      "now exercise performance and its ability\n",
      "to help us auto regulate volume\n",
      "so i'm going to talk about what within\n",
      "accumulation phase performances\n",
      "and why we should care a little bit\n",
      "technical we'll explain why we're going\n",
      "to talk about using performance as a\n",
      "partial tool to inform volume\n",
      "manipulations and of course\n",
      "we're going to use performance to\n",
      "actually auto regulate volume\n",
      "raising or lowering the amount of\n",
      "training we're doing week to week to\n",
      "week to make sure we get our best\n",
      "hypertrophy\n",
      "outcomes so here's the deal what the\n",
      "hell is within accumulation performance\n",
      "why do we care\n",
      "accumulation phase is week one all the\n",
      "way to whatever week four\n",
      "six eight whatever before your deload\n",
      "okay\n",
      "the change in performance how many sets\n",
      "or so how many reps\n",
      "and loads you're getting per set is that\n",
      "performance change within the\n",
      "accumulation so like week one compared\n",
      "to week two do we three week four week\n",
      "five etc\n",
      "not between mesoaccumulation or sorry\n",
      "between meso performance which is like\n",
      "okay\n",
      "my pr and this mesocycle was this and\n",
      "then d load in the next mesocycle what's\n",
      "my pr\n",
      "that's different so here we're talking\n",
      "about just within a single accumulation\n",
      "phase\n",
      "how does our performance essentially our\n",
      "rep strength how does our rep strength\n",
      "change throughout and how quickly it\n",
      "changes how it changes can help us\n",
      "inform\n",
      "what we should do with our training\n",
      "volume now the performance\n",
      "is complex because at least two factors\n",
      "feed into how you perform every single\n",
      "time one\n",
      "is of course fitness which means maybe\n",
      "your muscle size is increasing\n",
      "it should be so you're getting higher\n",
      "performances from that maybe you're\n",
      "metabolically getting better at doing\n",
      "the lift\n",
      "and you know there's more oxygen or\n",
      "wastes are leaving faster you can get\n",
      "more reps that way a little bit more\n",
      "load same reps\n",
      "and of course neural and technical\n",
      "abilities like your nervous system can\n",
      "be\n",
      "really figuring out how to do the lift\n",
      "your technique improves all of a sudden\n",
      "your performance is going up\n",
      "that's what makes performance go up what\n",
      "leads performance to go down or pulls it\n",
      "down or from any one\n",
      "point is fatigue that accumulates\n",
      "throughout the mesocycle eventually to\n",
      "unsustainable levels but yeah you get\n",
      "tired too not all that tiredness goes\n",
      "away by the next session\n",
      "so while you are getting better fatigue\n",
      "is pulling you down in\n",
      "between those two factors is how\n",
      "performance is determined\n",
      "now ideally in an intelligently designed\n",
      "program\n",
      "you want to scale performance to reps in\n",
      "reserve so each week\n",
      "when you start with roughly three reps\n",
      "in reserve each week you should have a\n",
      "rough\n",
      "reps in reserve goal so for example just\n",
      "as\n",
      "an example you can you know week one is\n",
      "three reps in reserve goal\n",
      "week two is two reps in reserve then one\n",
      "rep then zero then deload\n",
      "or you can stretch them out more and\n",
      "have three three so week one and week\n",
      "two or both three\n",
      "week three and week four both two and\n",
      "then two weeks or one two weeks or zero\n",
      "and then you de-load something like that\n",
      "but just generally it should progress\n",
      "from three all the way to zero okay and\n",
      "during that time each week you should\n",
      "have kind of a pre-planned\n",
      "goal of how many reps or how much weight\n",
      "to add every single session or every\n",
      "single week rather\n",
      "so you say okay i'm gonna go from three\n",
      "or zero ir\n",
      "and during that time i'm gonna add like\n",
      "five pounds every time or a rep every\n",
      "time or two and a half pounds plus or up\n",
      "so on and so forth right\n",
      "the ultimate goal of a really solid\n",
      "accumulation phase a real\n",
      "messi cycle you look back on you're like\n",
      "man i did great\n",
      "is you hit your maximum recoverable\n",
      "volume\n",
      "right you're up to your volumes you\n",
      "can't possibly sustain\n",
      "and the same week in which you also plan\n",
      "to hit your\n",
      "zero rar lift so not only do you go all\n",
      "the way to failure\n",
      "but you do it with such high volume that\n",
      "next week you can't repeat it because it\n",
      "can easily be the other way around where\n",
      "you can go to failure for like three\n",
      "weeks in a row\n",
      "and still not hit your mrv if your\n",
      "volume is low enough so you can still\n",
      "match the same performance\n",
      "or even increase your performance\n",
      "ideally what we want is that same week\n",
      "to be the end week which means you're at\n",
      "your mrv and zero\n",
      "are at the same time peak week it's a\n",
      "terrible time but it lets you know you\n",
      "put everything you possibly could into\n",
      "your hypertrophy training\n",
      "deload because you deserve it and you\n",
      "need it and then you restart the\n",
      "progression again so\n",
      "this is the universe in which we're\n",
      "going to be speaking about how to track\n",
      "performance\n",
      "given that we're doing all this\n",
      "we have to consider you know how we're\n",
      "going to change volume\n",
      "right so why do we care\n",
      "why do we care about this if you add\n",
      "volume\n",
      "way too quickly you're going to reach\n",
      "your mrv\n",
      "way before you are attempting to do it\n",
      "right if you don't increase your volume\n",
      "quickly enough\n",
      "you can go weeks and weeks training all\n",
      "the way to failure which is probably\n",
      "suboptimal with stimulus to fatigue\n",
      "ratio\n",
      "but not hit your mrv because you're just\n",
      "training too little\n",
      "right in order to stay on track and have\n",
      "the best mezzo possible\n",
      "you want some kind of intermediate level\n",
      "of volume adjustment so that your volume\n",
      "isn't too high it's not too low\n",
      "it's slowly rising and then it peaks at\n",
      "the same time as you get\n",
      "zero reps in reserve and you've had a\n",
      "nice long time\n",
      "at least three weeks possibly more like\n",
      "four or more\n",
      "to train consecutively right so that's\n",
      "the deal\n",
      "now this performance so we're going to\n",
      "be tracking performance and trying to\n",
      "change volume based on how we perform\n",
      "every session\n",
      "it's not a complete tool there's lots of\n",
      "erroneous stuff that can happen here\n",
      "it's only a partial tool it can't tell\n",
      "you for sure\n",
      "if you're stimulating muscle growth it\n",
      "can't tell you how much muscle growth\n",
      "and other signs have to be used\n",
      "like the pump or like disruption but it\n",
      "can be an important indicator\n",
      "let's find out how it works the\n",
      "two things we need for this to work is\n",
      "set quality you have to have good\n",
      "technique\n",
      "and you have to be pushing it hard if\n",
      "you're not doing that then how the hell\n",
      "can you tell\n",
      "if you're supposed to be adding volume\n",
      "or subtracting it or maybe just focusing\n",
      "on better set quality\n",
      "can leave you you know with half the\n",
      "volume that you should have planned\n",
      "before\n",
      "right four amazing sets of leg press can\n",
      "be the same hypertrophic stimulus as\n",
      "eight shitty sets of leg press\n",
      "you don't wanna be doing eight sets and\n",
      "be like i guess i gotta do 10 next week\n",
      "because i barely got sore my performance\n",
      "is really\n",
      "way too easy you don't want to do that\n",
      "so quality first\n",
      "and minimal load progression has to be\n",
      "in place or rep progression\n",
      "or both which is to say the goal should\n",
      "be\n",
      "to add at least like one rep every\n",
      "session or some load to the working sets\n",
      "or some combination of both every single\n",
      "week so you have to be pushing it the\n",
      "entire time\n",
      "if that's the case how do we auto\n",
      "regulate performance\n",
      "uh or sorry auto regulate volume based\n",
      "on performance\n",
      "okay if in order to have\n",
      "a challenging workout sufficiently\n",
      "challenging\n",
      "from one week to another in another way\n",
      "in other words in order to hit your\n",
      "rough\n",
      "rir target you need to increase the\n",
      "amount of load or the amount of reps by\n",
      "a ton\n",
      "one thing we can tell is that you\n",
      "probably didn't accumulate a whole lot\n",
      "of fatigue in the week before\n",
      "which is another way of saying there's a\n",
      "chance your volume was too low\n",
      "so let's say you normally train with\n",
      "like five or six sets\n",
      "of a shoulder press you do two sets of\n",
      "shoulder press in the first week or\n",
      "let's say one\n",
      "and then next week you've got to like\n",
      "add way more weight to the bar or more\n",
      "reps than you were expecting\n",
      "because otherwise it's just not anywhere\n",
      "close they are it's not challenging\n",
      "well it's not challenging because one\n",
      "fucking set didn't hardly do shit it\n",
      "didn't make you fatigued at all\n",
      "that's good but also it's just one set\n",
      "so it probably\n",
      "didn't cost a lot of hypertrophy if you\n",
      "want to like hit many pr's week after\n",
      "week after week and have them be\n",
      "meaningless\n",
      "and cause you some good neural gains but\n",
      "no actual size gains just keep the\n",
      "volume super low that's how power\n",
      "lifters train and they're super fucking\n",
      "strong for their size\n",
      "but there's a reason they're not the\n",
      "most jacked people in the world and if\n",
      "they want to become\n",
      "the most jack people when powerless\n",
      "transition to bodybuilders they do more\n",
      "volume\n",
      "which makes them tired more often which\n",
      "means they can't hit these crazy pr's\n",
      "all the time\n",
      "but they can hit the small ones and get\n",
      "amazing work because the volume's high\n",
      "enough so if you're way over performing\n",
      "week to week to week in other words if\n",
      "like this week is way\n",
      "you hit all your targets way too easy or\n",
      "in order for you to actually hit your\n",
      "targets\n",
      "successfully you have to put way more\n",
      "weight or do more way more reps\n",
      "you can be not positive but you can at\n",
      "least suspect\n",
      "that the week before you fucking didn't\n",
      "do enough right your fatigue is not high\n",
      "enough and that's why that's happened\n",
      "if performance is on track okay the reps\n",
      "in reserve\n",
      "you plan to load progression and\n",
      "reprogression you hit exactly the rar\n",
      "that you probably thought you wanted so\n",
      "for example\n",
      "first week is three r you do a good\n",
      "amount of volume next week you do same\n",
      "amount of volume or maybe a little more\n",
      "or something\n",
      "and you do one rep more on each set and\n",
      "five times more in each set and you hit\n",
      "exactly like\n",
      "we're somewhere between two and three r\n",
      "you're fucking golden\n",
      "don't touch anything that's probably the\n",
      "same amount of volume is going to do\n",
      "good you may under some conditions\n",
      "be able to increase the volume just a\n",
      "little bit by one set but not by much\n",
      "because you're on track don't break the\n",
      "shit that works you're on track it's all\n",
      "going really well\n",
      "on the other hand if\n",
      "you have to violate your rar goal in\n",
      "order to match your numbers\n",
      "okay and we do nrp recommend\n",
      "that you stick to your number so like if\n",
      "you squat at 200 for sets of 10 last\n",
      "week\n",
      "this week is 205 for sets of 10 or sets\n",
      "of 11 or whatever you had planned\n",
      "don't just like make it easier because\n",
      "no i have to hit r2 rar is real tough to\n",
      "guess\n",
      "but plan progressions are really easy to\n",
      "follow follow your plan progression\n",
      "and if following it you know your rer is\n",
      "lower than it was supposed to be\n",
      "man that means up to that point you've\n",
      "had a lot of volume\n",
      "and probably the week before was a fuck\n",
      "load of volume it fatigued you\n",
      "and it made that goal harder to reach\n",
      "than it should have been\n",
      "a lot of other stuff can be wrong which\n",
      "is why this is only a partial tool\n",
      "but if you have a week in which you\n",
      "barely struggle to hit your numbers and\n",
      "like you kind of hit them but your rirs\n",
      "were a little lower than you wanted and\n",
      "you've got more weeks of training to do\n",
      "almost certainly do not increase the\n",
      "volume either keep the volume the same\n",
      "or potentially reduce it a little bit\n",
      "lastly\n",
      "if you actually reach failure and\n",
      "underperform so for example\n",
      "you did a 100 pounds for 10 reps last\n",
      "week but a fuck load of sets of 10.\n",
      "you get so fatigued that this week at\n",
      "105 pounds i mean you're expecting for\n",
      "equivalency\n",
      "at least nine reps you get seven reps\n",
      "at 105 and then the bar falls on you and\n",
      "you're like fuck that's it that's\n",
      "failure\n",
      "right if you truly underperform you're\n",
      "pretty much\n",
      "at by definition at mrv and especially\n",
      "if that repeats in another session later\n",
      "in the week\n",
      "you need to execute some sort of\n",
      "recovery strategy asap that may be a\n",
      "recovery session\n",
      "recovery half week and if you if you\n",
      "youtube google all these that we've got\n",
      "videos on them\n",
      "um or a deload okay that means you just\n",
      "pushed it way too hard\n",
      "here's the fucked up thing about fatigue\n",
      "it accumulates and once it's there\n",
      "you just gotta deal with it right making\n",
      "a mess is a lot easier than cleaning up\n",
      "the mess\n",
      "so volume progressions should by\n",
      "definition almost be conservative\n",
      "like if you think you can go up by two\n",
      "sets but maybe one just go up by one\n",
      "because if you get into this situation\n",
      "uh for point d here\n",
      "and it's too much man you can't just be\n",
      "like i'll just do less volume next week\n",
      "well no you're already over\n",
      "mrv your fatigue is sky high next week\n",
      "you won't hit a pr\n",
      "then after that you won't hit a pr\n",
      "you're just in the shitty dead zone of\n",
      "being way too tired to do anything\n",
      "you have to back away you have to deload\n",
      "which takes time away from super\n",
      "productive\n",
      "maximally productive hypertrophy\n",
      "training so if you're a point a\n",
      "not bad but you need to up the ante\n",
      "probably do a little more volume next\n",
      "week\n",
      "if you're at point b fucking golden\n",
      "don't touch anything if you're at point\n",
      "c\n",
      "you really sleep a ton really eat super\n",
      "well this next week\n",
      "definitely don't increase your volume\n",
      "and potentially when you're increasing\n",
      "your load or reps increase them very\n",
      "little next week because you're on that\n",
      "fine line you're on that edge and maybe\n",
      "consider actually reducing your volume a\n",
      "little bit\n",
      "so that you don't wash yourself and\n",
      "fatigue too soon so for example\n",
      "if you reach point c and it's week\n",
      "number two and you want to go for four\n",
      "weeks\n",
      "you don't just have to think about week\n",
      "three you have to think about week four\n",
      "two\n",
      "so if you did six sets of let's say\n",
      "curls and now you're at point c\n",
      "in week three so in week three is you\n",
      "did six sets of curls in week two\n",
      "and now it's week three how many sets\n",
      "should you do like if you do six\n",
      "you may actually overreach right there\n",
      "and then week four is a fucking wash\n",
      "because you won't be able to hit your\n",
      "performance goals\n",
      "so maybe yeah if you eat a lot of sleep\n",
      "a lot whatever and really not up you can\n",
      "do six definitely don't do seven or\n",
      "eight sets\n",
      "but there's some logic to doing like\n",
      "four or five\n",
      "you do four or five and week three and\n",
      "then you get good good uh prs\n",
      "you you break your records on how many\n",
      "reps you did or how much weight or both\n",
      "and then week four you go back up to six\n",
      "and then\n",
      "really spill it because it doesn't\n",
      "matter weak force last week anyway so\n",
      "again playing the conservative game\n",
      "almost always pays off right\n",
      "that's it for using performance again\n",
      "just very general and mostly works at\n",
      "the extremes\n",
      "in the next video we'll talk about how\n",
      "to combine pump\n",
      "disruption and performance indicators to\n",
      "start now really painting a much clearer\n",
      "picture of your training volume\n",
      "and really helping you make much better\n",
      "decisions about whether to stay the\n",
      "course\n",
      "in number of sets you do next week\n",
      "increase or decrease\n",
      "see you next time\n"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "def retrieve(query, index, data, top_k=5):\n",
    "    # Transforms the user query into a vector \n",
    "    query_embedding = model.encode([query])\n",
    "    # Index search. It returns the distances and the indeces of the closest articles in the dataset\n",
    "    distances, indices = index.search(np.array(query_embedding), top_k)\n",
    "        \n",
    "    # it extracts the 'content' of the most relevant using the indeces and its video ID \n",
    "    article = data.loc[indices.tolist()[0][0], 'content']\n",
    "    res_id = data.loc[indices.tolist()[0][0], 'title']\n",
    "    video = \"https://www.youtube.com/watch?v=\" + res_id  \n",
    "    \n",
    "    yt = YouTube(video)\n",
    "    # channel_id = yt.channel_id\n",
    "    # channel_url = yt.channel_url\n",
    "    vid_author = yt.author\n",
    "    vid_title = yt.title\n",
    "        \n",
    "    return vid_author, vid_title, article\n",
    "\n",
    "query = \"I've done a deload week and I want to go back to my training. What's the ideal volume for the first week of the mesocycle?\"\n",
    "retrieved_articles = retrieve(query, index, data)\n",
    "for article in retrieved_articles:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised training loop\n",
    "We lack the labeled data to train the QA bot. Let's pretrain using the corpus. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 2,
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
=======
      "Map: 100%|██████████| 1161749/1161749 [08:41<00:00, 2228.84 examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 66\u001b[0m\n\u001b[0;32m     58\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     59\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     60\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     61\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     62\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdatasets,\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\transformers\\trainer.py:2230\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2231\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\accelerate\\data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\datasets\\dataset_dict.py:81\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     77\u001b[0m available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     78\u001b[0m     split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     79\u001b[0m ]\n\u001b[0;32m     80\u001b[0m suggested_split \u001b[38;5;241m=\u001b[39m available_suggested_splits[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m available_suggested_splits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please first select a split. For example: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`my_dataset_dictionary[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\""
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "SUBSAMPLE = True \n",
    "\n",
    "# Load the LLaMA model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Set special padding \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Pre-train the model based on the corpus \n",
    "file_path = \"train_data.txt\"\n",
    "datasets = load_dataset('text', data_files={'train': file_path})\n",
    "\n",
    "# Tokenize the dataset \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        return_special_tokens_mask=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"]) \n",
    "\n",
    "# Reduce the dataset size by taking only a subset\n",
    "def split_dataset(dataset, fraction):\n",
    "    split_index = int(len(dataset) * fraction)\n",
    "    return dataset.select(range(split_index))\n",
    "\n",
    "# Use 50% of the dataset to speed up training\n",
    "if SUBSAMPLE:\n",
    "    fraction = 0.1\n",
    "else: \n",
    "    fraction = 1\n",
    "train_subset = split_dataset(tokenized_datasets['train'], fraction)   \n",
    "\n",
    "# Create batches from the training dataset \n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_subset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the model changes before and after fine-tunning"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 8,
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "c:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
=======
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning:\n",
      "=============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the most effective exercise to hit the long head of the triceps?\n",
      "Answer: What is the most effective exercise to hit the long head of the triceps?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: I've been doing a delooad for one week and now I want to go back to my training. What should be the ideal volume for the first mesocycle?\n",
<<<<<<< HEAD
      "Answer: I've been doing a delooad for one week and now I want to go back to my training. What should be the ideal volume for the first mesocycle?\n",
      "\n",
      "I have been doing a delooad for a week and now\n",
=======
      "Answer: I've been doing a delooad for one week and now I want to go back to my training. What should be the ideal volume for the first mesocycle? I'm not sure I'll be able to do it. I'm not\n",
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: During a deload phase should I decrease my volume, my intensity or both?\n",
      "Answer: During a deload phase should I decrease my volume, my intensity or both?\n",
      "\n",
      "\n",
<<<<<<< HEAD
      "I’m not sure.\n",
      "I’m not sure.\n",
      "I’m not sure.\n",
      "I’m not sure.\n",
=======
      "\n",
      "I’m not sure if I’m going to be able to do that.\n",
      "I’m not sure if I’\n",
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the best approach to do progressive overload?\n",
<<<<<<< HEAD
      "Answer: What is the best approach to do progressive overload?\n",
      "\n",
      "\n",
      "The best approach to do progressive overload is to do a progressive overload.\n",
      "The best approach to do progressive overload is to do a progressive overload.\n",
      "The best approach to do progressive overload\n",
=======
      "Answer: What is the best approach to do progressive overload?”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
      "\n",
      "Question: Show me the correct technique for dumbell lateral raises\n",
      "Answer: Show me the correct technique for dumbell lateral raises.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "  0%|          | 0/3 [01:04<?, ?it/s]\n"
=======
      "\n",
      "  0%|          | 0/3 [09:05<?, ?it/s]\n"
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
<<<<<<< HEAD
      "Cell \u001b[1;32mIn[9], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Ask the same question after fine-tuning\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter fine-tuning:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=============================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
=======
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Ask the same question after fine-tuning\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter fine-tuning:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=============================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\transformers\\trainer.py:2230\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2231\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\accelerate\\data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\datasets\\dataset_dict.py:81\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     77\u001b[0m available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     78\u001b[0m     split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     79\u001b[0m ]\n\u001b[0;32m     80\u001b[0m suggested_split \u001b[38;5;241m=\u001b[39m available_suggested_splits[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m available_suggested_splits \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please first select a split. For example: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`my_dataset_dictionary[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m )\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']\""
     ]
    }
   ],
   "source": [
    "# QUESTION BATTERY \n",
    "question_battery = [\n",
    "    \"What is the most effective exercise to hit the long head of the triceps?\",\n",
    "    \"I've been doing a delooad for one week and now I want to go back to my training. What should be the ideal volume for the first mesocycle?\",\n",
    "    \"During a deload phase should I decrease my volume, my intensity or both?\",\n",
    "    \"What is the best approach to do progressive overload?\",\n",
<<<<<<< HEAD
    "    \"Show me the correct technique for dumbell lateral raises\",\n",
    "    \"What is the ideal rep range for squats?\"\n",
=======
    "    \"Show me the correct technique for dumbell lateral raises\"\n",
>>>>>>> 8313611a4e14e3bd4910126b964001f00352a37b
    "]\n",
    "\n",
    "# Function to generate answers before and after training\n",
    "def generate_answer(question):\n",
    "    inputs = tokenizer.encode(question, return_tensors='pt')\n",
    "    attention_mask = inputs.ne(tokenizer.pad_token_id).long()\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,  \n",
    "        top_k=50,         \n",
    "        top_p=0.9         \n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Ask a question before fine-tuning\n",
    "print(\"Before fine-tuning:\\n=============================\")\n",
    "for question in question_battery:\n",
    "    answer = generate_answer(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Ask the same question after fine-tuning\n",
    "print(\"After fine-tuning:\\n=============================\")\n",
    "# for question in question_battery:\n",
    "#     answer = generate_answer(question)\n",
    "#     print(f\"Question: {question}\")\n",
    "#     print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Integrate with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LangChain' from 'langchain' (c:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\langchain\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangChain\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaissRetriever\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceGenerator\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LangChain' from 'langchain' (c:\\Users\\dvall\\llm-solutions\\.venv\\lib\\site-packages\\langchain\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# from langchain import LangChain\n",
    "# from langchain.prompts import Prompt\n",
    "\n",
    "# prompt = Prompt.from_components(\"Query: {query}\\nContext: {context}\\nAnswer:\")\n",
    "# langchain = LangChain(retriever=retrieve, generator=generate_response, prompt=prompt)\n",
    "\n",
    "# response = langchain.run(query)\n",
    "# print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
